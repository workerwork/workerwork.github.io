<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>linux on WorkSpace</title>
    <link>https://workerwork.github.io/tags/linux/</link>
    <description>Recent content in linux on WorkSpace</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 11 Jan 2021 16:36:51 +0800</lastBuildDate>
    
	<atom:link href="https://workerwork.github.io/tags/linux/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>BUILD 5GC</title>
      <link>https://workerwork.github.io/posts/build-5gc/</link>
      <pubDate>Mon, 11 Jan 2021 16:36:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/build-5gc/</guid>
      <description>1. 目录结构 dongfeng@worker:~/work/5G/5gc-c-build$ tree . ├── build.sh ├── README.md └── source ├── 5gc-c.service ├── amf │ └── amf1 │ ├── bin │ │ ├── amf -&amp;gt; amf_1.0.0 │ │ ├── amf_1.0.0 │ │ ├── lo.bin │ │ └── ls.bin │ ├── config │ │ ├── amf.conf │ │ └── asn.log.properties │ ├── log │ │ └── amf.log │ └── xml │ ├── amfInfo.xml │ ├── ausf.xml │ ├── guami.xml │ ├── nrf.</description>
    </item>
    
    <item>
      <title>linux checksum</title>
      <link>https://workerwork.github.io/posts/checksum/</link>
      <pubDate>Mon, 11 Jan 2021 09:49:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/checksum/</guid>
      <description>1. 关键结构 由于目前很多网卡设备是支持对L4层数据包进行校验和的计算和验证的，所以在L4协议软件的实现中，
会根据网卡的支持情况作不同的处理，为此内核在struct sk_buff结构和struct net_device中增加了校验和相关的参数，如下：
struct sk_buff
上面的结构中，和校验和有关的几个字段如下：
#define CHECKSUM_NONE 0 #define CHECKSUM_UNNECESSARY 1 #define CHECKSUM_COMPLETE 2 #define CHECKSUM_PARTIAL 3 struct sk_buff { union { __wsum	csum; struct { __u16	csum_start; __u16	csum_offset; }; }; __u8 ip_summed:2, }  联合体中哪个成员有效取决于ip_summed的值，ip_summed共两个bit，可取四个标志，而且在发送和接收时的含义还有所不同
在接收过程中，ip_summed字段包含了设备驱动告诉L4软件当前校验和的状态，各取值含义如下：
 CHECKSUM_NONE：硬件没有提供校验和，可能是硬件不支持，也可能是硬件校验出错但是并未丢弃数据包，而是让L4软件重新校验 CHECKSUM_UNNECESSARY：硬件已经进行了完整的校验，无需软件再进行检查，L4收到数据包后如果检查ip_summed是这种情况，就可以跳过校验过程 CHECKSUM_COMPLETE：硬件已经校验了L4报头和其payload部分，并且校验和保存在了csum中，L4软件只需要再计算伪报头然后检查校验结果即可  在发送过程中，ip_summed字段包含了L4软件告诉设备驱动程序当前校验和的状态，各取值含义如下：
 CHECKSUM_NONE：L4软件已经进行了校验，硬件无需做任何事情 CHECKSUM_PARTIAL：L4软件计算了伪报头，并且将值保存在了首部的check字段中，硬件需要计算其余部分的校验和  struct net_device
net_device结构中的feature字段中定义了如下和校验和相关的字段，这些字段表明了硬件计算校验和的能力
NETIF_F_NO_CSUM：该设备非常可靠，无需L4执行任何校验，环回设备一般设置该标记 NETIF_F_IP_CSUM：设备可以对基于IPv4的TCP和UDP数据包进行校验 NETIF_F_IPV6_CSUM：设备可以对基于IPv6的TCP和UDP数据包进行校验 NETIF_F_HW_CSUM： 设备可以对任何L4协议的数据包进行校验
注：这些概念和字段的含义同样适用于TCP校验和处理过程
2. 输入数据报的校验和计算 udp4_csum_init()
@skb: 待校验的数据报 @uh：该数据报的UDP首部 @proto：L4协议号，为IPPROTO_UDP或者IPPROTO_UDPLITE static inline int udp4_csum_init(struct sk_buff *skb, struct udphdr *uh, int proto) { const struct iphdr *iph; int err; //这两个字段用于指示对报文的哪些部分进行校验，cov指coverage， //只有UDPLite使用，对于UDP，会对整个报文进行校验 UDP_SKB_CB(skb)-&amp;gt;partial_cov = 0; UDP_SKB_CB(skb)-&amp;gt;cscov = skb-&amp;gt;len; //UDPLITE，忽略 if (proto == IPPROTO_UDPLITE) { err = udplite_checksum_init(skb, uh); if (err) return err; } iph = ip_hdr(skb); //UDP首部校验和字段为0，这种情况说明已经处理过了，设置为CHECKSUM_UNNECESSARY，后续无需再进行处理 if (uh-&amp;gt;check == 0) { skb-&amp;gt;ip_summed = CHECKSUM_UNNECESSARY; } else if (skb-&amp;gt;ip_summed == CHECKSUM_COMPLETE) { //还有伪首部需要校验，所以添加伪首部校验，如果校验成功，设置为CHECKSUM_UNNECESSARY //csum_tcpudp_magic()计算伪首部校验和后进行验证，如果验证ok，返回0，该函数体系结构相关， //为了高效，用汇编语言实现 if (!</description>
    </item>
    
    <item>
      <title>二进制瘦身</title>
      <link>https://workerwork.github.io/posts/sub-bin/</link>
      <pubDate>Thu, 07 Jan 2021 11:06:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/sub-bin/</guid>
      <description>1. 符号表信息和调试信息 符号表信息（symbols）和调试信息（debug info）是由不同段区分的。
使用 readelf -S binfile 可以查看ELF文件的所有段。
调试信息相关的段：
# readelf -S a.out | grep debug [27] .debug_aranges PROGBITS 0000000000000000 000016d0 [28] .debug_info PROGBITS 0000000000000000 00001700 [29] .debug_abbrev PROGBITS 0000000000000000 00001a0f [30] .debug_line PROGBITS 0000000000000000 00001adb [31] .debug_str PROGBITS 0000000000000000 00001bd2  符号表相关的段：
# readelf -S a.out | grep tab [32] .symtab SYMTAB 0000000000000000 00001e18 [33] .strtab STRTAB 0000000000000000 00002670 [34] .shstrtab STRTAB 0000000000000000 00002a8f  注： 下文中提及的符号表相关段将不包括 .</description>
    </item>
    
    <item>
      <title>EXPORT SYMBOL</title>
      <link>https://workerwork.github.io/posts/export-symbol/</link>
      <pubDate>Wed, 30 Dec 2020 15:05:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/export-symbol/</guid>
      <description>1. 背景 EXPORT_SYMBOL只出现在2.6内核中，在2.4内核默认的非static 函数和变量都会自动导入到kernel 空间的， 都不用EXPORT_SYMBOL() 做标记的。2.6就必须用EXPORT_SYMBOL() 来导出来（因为2.6默认不导出所有的符号）
2. EXPORT_SYMBOL的作用 EXPORT_SYMBOL标签内定义的函数或者符号对全部内核代码公开， 不用修改内核代码就可以在您的内核模块中直接调用，即使用EXPORT_SYMBOL可以将一个函数以符号的方式导出给其他模块使用
EXPORT_SYMBOL(符号名); 可以导出static函数到符号表中 EXPORT_SYMBOL_GPL(符号名) //用于GPL协议认证的模块 EXPORT_SYMBOL_GPL的符号必须要用MODULE_LICENSE(&amp;quot;GPL&amp;quot;)或者用MODULE_LICENSE(&amp;quot;Dual BSD/GPL&amp;quot;)之后才能在模块中引用。 而且MODULE_LICENSE(&amp;quot;char&amp;quot;)中的char不可以是任意字符，否则错误和没有MODULE_LICENSE效果一样。  这里要和System.map做一下对比： System.map 中的是连接时的函数地址。连接完成以后，在2.6内核运行过程中，是不知道哪个符号在哪个地址的。 EXPORT_SYMBOL 的符号，是把这些符号和对应的地址保存起来，在内核运行的过程中，可以找到这些符号对应的地址。 而模块在加载过程中，其本质就是能动态连接到内核，如果在模块中引用了内核或其它模块的符号， 就要EXPORT_SYMBOL这些符号，这样才能找到对应的地址连接
3. 使用方法  在模块函数定义之后使用EXPORT_SYMBOL（函数名） 在掉用该函数的模块中使用extern对之声明 首先加载定义该函数的模块，再加载调用该函数的模块  另外，在编译调用某导出函数的模块时，往往会有WARNING: &amp;ldquo;＊＊＊＊&amp;rdquo; [＊＊＊＊＊＊＊＊＊＊] undefined! 使用dmesg命令后会看到相同的信息。开始我以为只要有这个错误就不能加载模块，后来上网查了一下， 发现这主要是因为在编译连接的时候还没有和内核打交道，当然找不到symbol了，但是由于你生成的是一个内核模块， 所以LD不提示error，而是给出一个warning，寄希望于在insmod的时候，内核能够把这个symbol连接上
1.EXPORT_SYMBOL EXPORT_SYMBOL( my_pub_func); 在预编译阶段会解析为: extern void *__crc_my_pub_func __attribute__((weak)); static const unsigned long __kcrctab_my_pub_func __attribute__((__used__)) __attribute__((section(&amp;quot;__kcrctab&amp;quot; &amp;quot;&amp;quot;), unused)) = (unsigned long) &amp;amp;__crc_my_pub_func; static const char __kstrtab_my_pub_func[] __attribute__((section(&amp;quot;__ksymtab_strings&amp;quot;))) = &amp;quot;&amp;quot; &amp;quot;my_pub_func&amp;quot;; static const struct kernel_symbol __ksymtab_my_pub_func __attribute__((__used__)) __attribute__((section(&amp;quot;__ksymtab&amp;quot; &amp;quot;&amp;quot;), unused)) = { (unsigned long)&amp;amp;my_pub_func, __kstrtab_my_pub_func }; 很显然__ksymtab_my_pub_func存储了my_pub_func的地址和符号信息,该符号对应的地址 只有insmod后才会确定; __ksymtab_my_pub_func会链接到__ksymtab section,__ksymtab section中的所有内容就构成了 内核&amp;quot;导出&amp;quot;的符号表,这个表在insmod 时候会用到.</description>
    </item>
    
    <item>
      <title>vpp和linux内核协议栈通信</title>
      <link>https://workerwork.github.io/posts/vpp-kernel/</link>
      <pubDate>Wed, 30 Dec 2020 14:30:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-kernel/</guid>
      <description>1. 背景介绍 基于VPP开发一款网络设备，除使用VPP框架实现端口数据报文转发外，还需要对设备进行配置和管理， 比较常用的是要实现ssh、web等相关服务，另外，由于VPP并未实现dhcp server的功能，控制面可能还需移植isc-dhcp类似的dhcp server。 相关的用户态程序是基于Linux内核的socket来收发包的，因此，需要实现VPP和Linux内核协议栈通信的方法， 可以将访问控制面的报文由VPP上送至Linux协议的协议栈。
2. 实现原理 VPP和Linux内核协议栈通信有两种方法，分别是基于Linux的TAP/TUN和VETH两种机制， 对这两种机制不了解的朋友，建议先阅读下如下两个衔接的内容：
TUN/TAP：https://segmentfault.com/a/1190000009249039
VETH：https://segmentfault.com/a/1190000009251098
基于TAP/TUN的方法：
如上图，PC的地址是192.168.1.2，Linux Kernel的管理接口地址是192.168.1.1，需要PC通过VPP可以ping同Linux内核的192.168.1.1地址
技术原理图如上，物理网卡对应VPP中的GE4/0/0，VPP创建tap接口，使用l2 bridge机制将GE4/0/0和tap桥接，进入GE4/0/0的报文通过l2 bridge转发到TAP， VPP的TAP相当于用户态进程，另一端对应的是Linux内核中的TAP接口，报文通过TAP机制重入到Linux内核，实现了VPP到控制面的通信
实现命令:
vpp#set int l2 bridge GigabitEthernet4/0/0 1 vpp#set int state GigabitEthernet4/0/0 up vpp#tap connect lstack vpp#set int l2 bridge tap-0 1 vpp#set int state tap-0 up  上述操作完成后，在linux后台，ifconfig发现多了一个lstack接口，给lstack接口配置好IP地址，就可以实现PC和设备的通信了。
基于VETH方法：
VETH技术原理如上图，通过在vpp上创建host-interface的方式实现，使用VETH方式的效率更高，实际使用时推荐此种方法
配置步骤如下
linux后台配置： ~# ip netns add ns0 ~# ip link add vpp0 type veth peer name vethns0 ~# ip link set vethns0 netns ns0 ~# ip netns exec ns0 ip link set lo up ~# ip netns exec ns0 ip link set vethns0 up ~# ip netns exec ns0 ip addr add 192.</description>
    </item>
    
    <item>
      <title>xfrm框架</title>
      <link>https://workerwork.github.io/posts/xfrm/</link>
      <pubDate>Mon, 28 Dec 2020 15:20:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/xfrm/</guid>
      <description>1. 简介 IPsec协议帮助IP层建立安全可信的数据包传输通道。
当前已经有了如StrongSwan、OpenSwan等比较成熟的解决方案，而它们都使用了Linux内核中的XFRM框架进行报文接收发送.
XFRM的正确读音是transform(转换), 这表示内核协议栈收到的IPsec报文需要经过转换才能还原为原始报文；
同样地，要发送的原始报文也需要转换为IPsec报文才能发送出去.
2. XFRM实例 IPsec中有两个重要概念：
安全关联(Security Association)和安全策略(Security Policy)，这两类信息都需要存放在内核XFRM。
核XFRM使用netns_xfrm这个结构来组织这些信息，它也被称为xfrm instance(实例)。
从它的名字也可以看出来，这个实例是与network namespace相关的，每个命名空间都有这样的一个实例，实例间彼此独立。
所以同一台主机上的不同容器可以互不干扰地使用XFRM
struct net { ...... #ifdef CONFIG_XFRM struct netns_xfrm	xfrm; #endif ...... }  3. Netlink通道 上面提到了Security Association和Security Policy信息，这些信息一般是由用户态IPsec进程(eg. StrongSwan)
下发到内核XFRM的，这个下发的通道在network namespace初始化时创建。
static int __net_init xfrm_user_net_init(struct net *net) { struct sock *nlsk; struct netlink_kernel_cfg cfg = { .groups	= XFRMNLGRP_MAX, .input	= xfrm_netlink_rcv, }; nlsk = netlink_kernel_create(net, NETLINK_XFRM, &amp;amp;cfg); ...... return 0; }  这样，当用户下发IPsec配置时，内核便可以调用 xfrm_netlink_rcv() 来接收.</description>
    </item>
    
    <item>
      <title>oom killer</title>
      <link>https://workerwork.github.io/posts/oom-killer/</link>
      <pubDate>Mon, 28 Dec 2020 11:20:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/oom-killer/</guid>
      <description>1. redis报错 MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.  redis数据不能写入磁盘了 修正方式：
1.改redis-conf配置文件中的 stop-writes-on-bgsave-error 为 no,保证redis正常运行 2.修改/etc/sysctl.conf 中vm.overcommit_memory 的值为 1，再使用sysctl -p使修改生效，然后重启redis overcommit_memory=0， 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。 overcommit_memory=1， 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。 overcommit_memory=2， 表示内核允许分配超过所有物理内存和交换空间总和的内存 3.查看磁盘占用情况，检查磁盘是否写满 4.查看内存占用，检查是否配额充足,有可能进程被oom-killer干掉了  2. oom-killer Linux 内核有个机制叫OOM killer(Out Of Memory killer)，该机制会监控那些占用内存过大， 尤其是瞬间占用内存很快的进程，然后防止内存耗尽而自动把该进程杀掉。 内核检测到系统内存不足、挑选并杀掉某个进程的过程可以参考内核源代码linux/mm/oom_kill.c， 当系统内存不足的时候，out_of_memory()被触发，然后调用select_bad_process()选择一个”bad”进程杀掉。 如何判断和选择一个”bad进程呢？linux选择”bad”进程是通过调用oom_badness()， 挑选的算法和想法都很简单很朴实：最bad的那个进程就是那个最占用内存的进程。  如何查看</description>
    </item>
    
    <item>
      <title>linux gcc编译</title>
      <link>https://workerwork.github.io/posts/gcc/</link>
      <pubDate>Fri, 25 Dec 2020 14:27:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/gcc/</guid>
      <description>1. 常用编译命令选项 假设源程序文件名为test.c 1. 无选项编译链接 用法：#gcc test.c 作用：将test.c预处理、汇编、编译并链接形成可执行文件。这里未指定输出文件，默认输出为a.out。 2. 选项 -o 用法：#gcc test.c -o test 作用：将test.c预处理、汇编、编译并链接形成可执行文件test。-o选项用来指定输出文件的文件名。 3. 选项 -E 用法：#gcc -E test.c -o test.i 作用：将test.c预处理输出test.i文件。 4. 选项 -S 用法：#gcc -S test.i 作用：将预处理输出文件test.i汇编成test.s文件。 5. 选项 -c 用法：#gcc -c test.s 作用：将汇编输出文件test.s编译输出test.o文件。 6. 无选项链接 用法：#gcc test.o -o test 作用：将编译输出文件test.o链接成最终可执行文件test。 7. 选项-O 用法：#gcc -O1 test.c -o test 作用：使用编译优化级别1编译程序。级别为1~3，级别越大优化效果越好，但编译时间越长。  2. 多源文件的编译方法 如果有多个源文件，基本上有两种编译方法： [假设有两个源文件为test.c和testfun.c] 1. 多个文件一起编译 用法：#gcc testfun.c test.c -o test 作用：将testfun.c和test.c分别编译后链接成test可执行文件。 2. 分别编译各个源文件，之后对编译后输出的目标文件链接。 用法： #gcc -c testfun.</description>
    </item>
    
    <item>
      <title>linux coredump设置</title>
      <link>https://workerwork.github.io/posts/core/</link>
      <pubDate>Thu, 17 Dec 2020 10:27:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/core/</guid>
      <description>1. core 在Linux下程序不寻常退出时，内核会在当前工作目录下生成一个core文件（是一个内存映像，同时加上调试信息，编译时需要加上 -g -Wall）。 使用gdb来查看core文件，可以指示出导致程序出错的代码所在文件和行数。  2. core文件的生成开关和大小限制 1.1 使用ulimit -c命令可查看core文件的生成开关 若结果为0，则表示关闭了此功能，不会生成core文件  1.2 使用ulimit -c filesize命令，可以限制core文件的大小（filesize的单位为kbyte） 如果生成的信息超过此大小，将会被裁剪，最终生成一个不完整的core文件或者根本就不生成。 如果生成被裁减的core文件，调试此core文件的时候，gdb也会提示错误。 用以下命令来表示core文件的大小不受限制. $ ulimit -c unlimited 用以下命令来阻止系统生成core文件: $ ulimit -c 0 备注:ulimit命令设置后只对一个终端有效，所以另起终端后需要重新设置。  3. 设置 Core Dump 的核心转储文件目录和命名规则 2.1 /proc/sys/kernel/core_uses_pid 可以控制产生的 core 文件的文件名中是否添加 pid 作为扩展 ， 文件内容为1，表示添加pid作为扩展名，生成的core文件格式为core.xxxx；为0则表示生成的core文件同一命名为core。 $ echo &amp;quot;1&amp;quot; &amp;gt; /proc/sys/kernel/core_uses_pid  2.2 /proc/sys/kernel/core_pattern 可以设置格式化的 core 文件保存位置或文件名 $ echo &amp;quot;/corefile/core-%e-%p-%t&amp;quot; &amp;gt; /proc/sys/kernel/core_pattern 说明:将会控制所产生的 core 文件会存放到 /corefile 目录下，产生的文件名为 core- 命令名 -pid- 时间戳 以下是参数列表: %p - insert pid into filename 添加pid %u - insert current uid into filename 添加当前uid %g - insert current gid into filename 添加当前gid %s - insert signal that caused the coredump into the filename 添加导致产生core的信号 %t - insert UNIX time that the coredump occurred into filename 添加core文件生成时的unix时间 %h - insert hostname where the coredump happened into filename 添加主机名 %e - insert coredumping executable name into filename 添加命令名  2.</description>
    </item>
    
    <item>
      <title>网卡offload GSO</title>
      <link>https://workerwork.github.io/posts/gso/</link>
      <pubDate>Mon, 30 Nov 2020 17:03:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/gso/</guid>
      <description> 1. GSO和TSO TSO(TCP Segmentation Offload): 是一种利用网卡来对大数据包进行自动分段，降低CPU负载的技术。 其主要是延迟分段
GSO(Generic Segmentation Offload): GSO是协议栈是否推迟分段，在发送到网卡之前判断网卡是否支持TSO，如果网卡支持TSO则让网卡分段，否则协议栈分完段再交给驱动
如果TSO开启，GSO会自动开启
驱动程序在注册网卡设备的时候默认开启GSO: NETIF_F_GSO
驱动程序会根据网卡硬件是否支持来设置TSO: NETIF_F_TSO
可以通过ethtool -K来开关GSO/TSO
2. 开启关系 GSO开启， TSO开启: 协议栈推迟分段，并直接传递大数据包到网卡，让网卡自动分段 GSO开启， TSO关闭: 协议栈推迟分段，在最后发送到网卡前才执行分段 GSO关闭， TSO开启: 同GSO开启， TSO开启 GSO关闭， TSO关闭: 不推迟分段，在tcp_sendmsg中直接发送MSS大小的数据包  对紧急数据包或GSO/TSO都不开启的情况，才不会推迟发送， 默认使用当前MSS
开启GSO后，tcp_send_mss返回mss和单个skb的GSO大小，为mss的整数倍
3. 包处理流程图 </description>
    </item>
    
    <item>
      <title>Linux 内核符号表</title>
      <link>https://workerwork.github.io/posts/system-map/</link>
      <pubDate>Mon, 30 Nov 2020 12:09:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/system-map/</guid>
      <description>1. 什么是符号(symbols) 什么是Symbol?
其实就是kernel中的变量(VariableName)或函数名称(Function Name)
这样可以方便程序员在写程序时可以直接参照这一份Symbol的索引文件，找到所需要的kernel信息，
这一份Symbol的索引文件又称为kernel symbol table
2. 内核符号表(Kernel Symbol Table) 内核符号表，就是在内核的内部函数或变量中，可供外部引用的函数和变量的符号表.
其实就是一个索引文件，它存在的目的就是让外部软件可以知道kernel文件内部实际分配的位置.
编译内核时，System.map文件用于存放内核符号表信息
System.map文件位于/或者/boot、/usr/src/linux/下
3. kallsyms 内核启动时候创建,供oops时定位错误，文件大小总为0，包含当前内核导出的、可供使用的变量或者函数；它只是内核数据的简单表示形式.
/proc/kallsyms是一个在启动时由Linux kernel实时产生的文件，当系统有任何变更时，它就会马上做出修正
可以理解为动态的符号表
4. 符号类型    符号类型 名称 说明     A Absolute 符号的值是绝对值，并且在进一步链接过程中不会被改变   B BSS 符号在未初始化数据区或区（section）中，即在BSS段中   C Common 符号是公共的。公共符号是未初始化的数据。在链接时，多个公共符号可能具有同一名称。如果该符号定义在其他地方，则公共符号被看作是未定义的引用   D Data 符号在已初始化数据区中   G Global 符号是在小对象已初始化数据区中的符号。某些目标文件的格式允许对小数据对象（例如一个全局整型变量）可进行更有效的访问   I Inderect 符号是对另一个符号的间接引用   N Debugging 符号是一个调试符号   R Read only 符号在一个只读数据区中   S Small 符号是小对象未初始化数据区中的符号   T Text 符号是代码区中的符号   U Undefined 符号是外部的，并且其值为0（未定义   - Stabs 符号是a.</description>
    </item>
    
    <item>
      <title>Linux 内核调试 kdump vmcore</title>
      <link>https://workerwork.github.io/posts/vmcore/</link>
      <pubDate>Wed, 28 Oct 2020 11:42:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vmcore/</guid>
      <description>1. kdump介绍 linux内核发送崩溃时，kdump会生成一个内核转储文件vmcore。 可以通过分析vmcore分析出内核崩溃的原因.
crash是一个被广泛应用的内核奔溃转储文件分析工具.
使用crash调试内核转储文件，需要安装crash工具和内核调试工具kernel-debuginfo.
2. 安装kdump crash kexec-tools 一般在系统镜像文件中就有相对应的rpm包
3. 配置kdump vim /boot/grub/menu.lst： 设置 crashkernel=auto vim /etc/kdump.conf： path /var/crash （core文件产生的目录）  4. 启动kdump systemctl start kdump  5. 安装kernel-debuginfo 下载内核版本对应的文件 kernel-debuginfo-3.10.0-957.el7.x86_64.rpm kernel-debuginfo-common-x86_64-3.10.0-957.el7.x86_64.rpm  6. 分析vmcore abrt-cli list crash /usr/lib/debug/lib/modules/3.10.0-957.el7.x86_64/vmlinux vmcore crash&amp;gt; bt PID: 7473 TASK: ffff9027d874bf40 CPU: 0 COMMAND: &amp;quot;cat&amp;quot; #0 [ffff9026d0ea3638] machine_kexec at ffffffffbd060b2a #1 [ffff9026d0ea3698] __crash_kexec at ffffffffbd113402 #2 [ffff9026d0ea3768] crash_kexec at ffffffffbd1134f0 #3 [ffff9026d0ea3780] oops_end at ffffffffbd717778 #4 [ffff9026d0ea37a8] no_context at ffffffffbd706f98 #5 [ffff9026d0ea37f8] __bad_area_nosemaphore at ffffffffbd70702f #6 [ffff9026d0ea3848] bad_area_nosemaphore at ffffffffbd7071a0 #7 [ffff9026d0ea3858] __do_page_fault at ffffffffbd71a730 #8 [ffff9026d0ea38c0] do_page_fault at ffffffffbd71a925 #9 [ffff9026d0ea38f0] page_fault at ffffffffbd716768 [exception RIP: strcmp+32] RIP: ffffffffbd353d20 RSP: ffff9026d0ea39a0 RFLAGS: 00010202 RAX: 000000000000002f RBX: ffff90240da5a080 RCX: 0000000000000000 RDX: 0000000000000000 RSI: 0000000000000001 RDI: ffff9026cd27fc11 RBP: ffff9026d0ea39a0 R8: 00000000004b1de2 R9: ffff9026cd27fc10 R10: ffff90253fc01d00 R11: ffffc0428c349fc0 R12: 0000000000000001 R13: ffff9026cd27fc10 R14: 0000000000000001 R15: ffff9027c16f1580 ORIG_RAX: ffffffffffffffff CS: 0010 SS: 0018 #10 [ffff9026d0ea39a8] send_log at ffffffffc0c22fd5 [xx] #11 [ffff9026d0ea3ac0] user_file at ffffffffc0c0c571 [xx] #12 [ffff9026d0ea3f00] sys_open at ffffffffc0c56670 [xxt] #13 [ffff9026d0ea3f50] system_call_fastpath at ffffffffbd71f7d5 RIP: 00007f2e860c2a30 RSP: 00007fff6d8755a8 RFLAGS: 00010202 RAX: 0000000000000002 RBX: 00007fff6d875868 RCX: 000000000060bc60 RDX: 1fffffffffff0000 RSI: 0000000000000000 RDI: 00007fff6d876293 RBP: 0000000000001000 R8: 0000000000000000 R9: 0000000000000000 R10: 00007fff6d875020 R11: 0000000000000246 R12: 0000000000402644 R13: 0000000000010000 R14: 0000000000000000 R15: 0000000000000000 ORIG_RAX: 0000000000000002 CS: 0033 SS: 002b crash&amp;gt; dis -l ffffffffbd353d20 /usr/src/debug/kernel-3.</description>
    </item>
    
    <item>
      <title>vpp-cli命令行总结</title>
      <link>https://workerwork.github.io/posts/vpp-cli/</link>
      <pubDate>Thu, 17 Sep 2020 17:37:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-cli/</guid>
      <description>1. 介绍 vpp网络协议栈配备了一组调试命令。访问CLI（具有适当权限）的最简单方法是使用vppctl命令:
sudo vppctl &amp;lt;cli command&amp;gt;  CLI解析器匹配静态的关键字字符串后，调用动作执行函数。 你可以通过在代码源文件中搜索VLIB_CLI_COMMAND宏的来查找CLI命令的源代码.
2. 调试和Telnet CLI 使用unix交互式参数或启动配置选项启用调试CLI。 这会导致VPP不以守护进程的情况启动，并在运行它的终端上显示命令行界面.
使用cli-listen localhost:5002选项启用Telnet CLI，这将导致VPP侦听localhost地址端口5002上的TCP连接。 然后，Telnet客户端可以连接到此端口（例如，telnet localhost 5002）并将收到命令行提示符。
以下配置将启用这两种机制：
unix { interactive cli-listen localhost:5002 }  CLI以横幅图形（可以禁用）和命令行提示符提示CLI开始。对于VPP的发布版本，命令行提示符通常为“vpp”， 对于启用了调试功能的开发版本，命令行提示符为“DBGvpp＃”,可以通过unix cli-prompt设置命令行提示符， 并通过unix cli-no-banner来禁止横幅.
3. CLI特征  &amp;lt;-或-&amp;gt; 左右光标键，在命令行内移动光标。 Ctrl-左/右将向左或向右搜索下一个单词的开头。 Home / end将光标跳转到行的开头和结尾。 可以使用exit命令关闭CLI。 或者，空输入行上的^ D也将关闭会话。关闭调试会话也将关闭VPP  4. 命令行参数与配置文件 VPP网络协议栈可以在命令行或配置文件中提供配置参数。 您可以通过搜索VLIB_CONFIG_FUNCTION宏在源代码中找到命令行参数解析器的相关代码。 调用VLIB_CONFIG_FUNCTION（foo_config，“foo”）将使函数foo_config接收名为“foo”的参数块中给出的所有参数， 例如：“foo {arg1 arg2 arg3 &amp;hellip;}”
VPP应用程序必须能够找到自己的可执行映像。确保这一点最简单方法是通过给出其绝对路径来调用VPP应用程序;
例如：/usr/bin/vpp 。
在启动时，VPP应用程序通过解析自己的ELF段以生成初始化，配置和退出处理程序的列表
配置文件：
还可以在启动配置文件中提供命令行参数，配置文件的路径在命令行上提供给VPP应用程序。
配置文件的格式是一个简单的文本文件，其内容与命令行相同，但是能够使用换行符使内容更加易于阅读。 例如：
unix { nodaemon /var/log/vpp/vpp.log full-coredump cli-listen localhost:5002 } api-trace { on } dpdk { dev 0000:03:00.</description>
    </item>
    
    <item>
      <title>Shared Memory</title>
      <link>https://workerwork.github.io/posts/shm/</link>
      <pubDate>Fri, 21 Aug 2020 09:22:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/shm/</guid>
      <description>1. Shared Memory介绍 共享内存是System V版本的最后一个进程间通信方式。共享内存，顾名思义就是允许两个不相关的进程访问同一个逻辑内存， 共享内存是两个正在运行的进程之间共享和传递数据的一种非常有效的方式。不同进程之间共享的内存通常为同一段物理内存。 进程可以将同一段物理内存连接到他们自己的地址空间中，所有的进程都可以访问共享内存中的地址。 如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程.
特别提醒：共享内存并未提供同步机制，也就是说，在第一个进程结束对共享内存的写操作之前， 并无自动机制可以阻止第二个进程开始对它进行读取，所以我们通常需要用其他的机制来同步对共享内存的访问，例如信号量.
2. C语言demo程序 //comm.h #ifndef _COMM_H__ #define _COMM_H__ #include&amp;lt;stdio.h&amp;gt; #include&amp;lt;sys/types.h&amp;gt; #include&amp;lt;sys/ipc.h&amp;gt; #include&amp;lt;sys/shm.h&amp;gt; #define PATHNAME &amp;quot;.&amp;quot; #define PROJ_ID 0x6666 int CreateShm(int size); int DestroyShm(int shmid); int GetShm(int size); #endif  //comm.c #include&amp;quot;comm.h&amp;quot; static int CommShm(int size,int flags) { key_t key = ftok(PATHNAME,PROJ_ID); if(key &amp;lt; 0) { perror(&amp;quot;ftok&amp;quot;); return -1; } int shmid = 0; if((shmid = shmget(key,size,flags)) &amp;lt; 0) { perror(&amp;quot;shmget&amp;quot;); return -2; } return shmid; } int DestroyShm(int shmid) { if(shmctl(shmid,IPC_RMID,NULL) &amp;lt; 0) { perror(&amp;quot;shmctl&amp;quot;); return -1; } return 0; } int CreateShm(int size) { return CommShm(size,IPC_CREAT | IPC_EXCL | 0666); } int GetShm(int size) { return CommShm(size,IPC_CREAT); }  //client.</description>
    </item>
    
    <item>
      <title>Unix domain socket</title>
      <link>https://workerwork.github.io/posts/uds/</link>
      <pubDate>Wed, 19 Aug 2020 15:29:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/uds/</guid>
      <description>1. Unix domain socket介绍 Unix domain socket 又叫 IPC(inter-process communication 进程间通信) socket，用于实现同一主机上的进程间通信. socket 原本是为网络通讯设计的，但后来在 socket 的框架上发展出一种 IPC 机制，就是 UNIX domain socket. 虽然网络 socket 也可用于同一台主机的进程间通讯(通过 loopback 地址 127.0.0.1)，但是 UNIX domain socket 用于 IPC更有效率：不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等，只是将应用层数据从一个进程拷贝到另一个进程. 这是因为，IPC机制本质上是可靠的通讯，而网络协议是为不可靠的通讯设计的. UNIX domain socket 是全双工的，API 接口语义丰富，相比其它 IPC 机制有明显的优越性，目前已成为使用最广泛的 IPC 机制， 比如 X Window 服务器和 GUI 程序之间就是通过 UNIX domain socket 通讯的. Unix domain socket 是 POSIX 标准中的一个组件，所以不要被名字迷惑，linux 系统也是支持它的.
2. C语言demo程序 下面是一个非常简单的服务器端程序，它从客户端读字符，然后将每个字符转换为大写并回送给客户端
#include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stddef.h&amp;gt; #include &amp;lt;sys/socket.</description>
    </item>
    
    <item>
      <title>video server</title>
      <link>https://workerwork.github.io/posts/video-server/</link>
      <pubDate>Wed, 19 Aug 2020 13:13:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/video-server/</guid>
      <description>1. SRS介绍 SRS/3.0，OuXuli，是一个流媒体集群，支持RTMP/HLS/WebRTC/SRT/GB28181，高效、稳定、易用，简单而快乐。 101 SRS is a RTMP/HLS/WebRTC/SRT/GB28181 streaming cluster, high efficiency, stable and simple.
项目地址: https://github.com/ossrs/srs
2. 运行docker容器启动srs服务 sudo docker run -d -p 1935:1935 -p 1985:1985 -p 8080:8080 ossrs/srs:3  3. 使用ffmpeg推流 需要安装ffmpeg
./pushflow.sh cat pushflow.sh #!/bin/bash while : do sudo ffmpeg -re -i dog.mp4 -vcodec copy -acodec copy -f flv -y rtmp://192.168.1.7:1935/live/livestream sleep 1 done  4. 终端使用vlc播放器拉流 视频地址填写: rtmp://192.168.1.7:1935/live/livestream
5. 使用live555支持rtsp 需要添加网关以指定监听地址 可以使用ffmpeg来转换视频源格式
ffmpeg -i dance.mp4 -codec copy -bsf: h264_mp4toannexb -f h264 dance.</description>
    </item>
    
    <item>
      <title>openssh8.1 rpm build</title>
      <link>https://workerwork.github.io/posts/openssh/</link>
      <pubDate>Fri, 14 Aug 2020 15:36:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/openssh/</guid>
      <description>1. 创建工作路径 mkdir -p /root/rpmbuild/{SOURCES,SPECS} cp openssh-8.1p1.tar.gz /root/rpmbuild/SOURCES/  2. 下载源码包 wget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.1p1.tar.gz  3. 制作准备 yum install rpm-build zlib-devel openssl-devel gcc perl-devel pam-devel unzip tar -zxf openssh-8.1p1.tar.gz cp ./openssh-8.1p1/contrib/redhat/openssh.spec . sed -i -e &amp;quot;s/%define no_x11_askpass 0/%define no_x11_askpass 1/g&amp;quot; openssh.spec sed -i -e &amp;quot;s/%define no_gnome_askpass 0/%define no_gnome_askpass 1/g&amp;quot; openssh.spec  4. 制作rpm包 rpmbuild -ba openssh.spec 如果出现 错误：构建依赖失败： openssl-devel &amp;lt; 1.1 被 ?? 需要 解决方法： vi openssh.spec 注释掉 BuildRequires: openssl-devel &amp;lt; 1.</description>
    </item>
    
    <item>
      <title>自建kvm虚拟机</title>
      <link>https://workerwork.github.io/posts/kvm2/</link>
      <pubDate>Wed, 22 Jul 2020 10:35:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/kvm2/</guid>
      <description>1. 自定义虚拟机 egrep -q &amp;quot;(svm|vmx)&amp;quot; /proc/cpuinfo &amp;amp;&amp;amp; echo &amp;quot;yes&amp;quot; lsmod|grep kvm # centos yum install -y qemu-kvm #KVM主程序，KVM虚拟化模块 yum install -y libvirt #虚拟化服务库 yum install -y bridge-utils ln -s /usr/libexec/qemu-kvm /usr/bin/qemu-kvm # ubuntu apt install qemu qemu-kvm apt install libvirt-bin apt install bridge-utils ln -s /usr/bin/qemu-system-x86_64 /usr/bin/qemu-kvm systemctl start libvirtd systemctl enable libvirtd qemu-img create -f qcow2 vm_NGC.img 20G #创建磁盘镜像 qemu-kvm -name vm_NGC -m 4096 -cpu host -enable-kvm -smp 8 -hda vm_NGC.</description>
    </item>
    
    <item>
      <title>DPDK-Devbind</title>
      <link>https://workerwork.github.io/posts/dpdk-devbind/</link>
      <pubDate>Tue, 30 Jun 2020 17:18:27 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/dpdk-devbind/</guid>
      <description>dpdk-devbind.py #! /usr/bin/env python # SPDX-License-Identifier: BSD-3-Clause # Copyright(c) 2010-2014 Intel Corporation # import sys import os import getopt import subprocess from os.path import exists, abspath, dirname, basename # The PCI base class for all devices network_class = {&#39;Class&#39;: &#39;02&#39;, &#39;Vendor&#39;: None, &#39;Device&#39;: None, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} encryption_class = {&#39;Class&#39;: &#39;10&#39;, &#39;Vendor&#39;: None, &#39;Device&#39;: None, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} intel_processor_class = {&#39;Class&#39;: &#39;0b&#39;, &#39;Vendor&#39;: &#39;8086&#39;, &#39;Device&#39;: None, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} cavium_sso = {&#39;Class&#39;: &#39;08&#39;, &#39;Vendor&#39;: &#39;177d&#39;, &#39;Device&#39;: &#39;a04b,a04d&#39;, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} cavium_fpa = {&#39;Class&#39;: &#39;08&#39;, &#39;Vendor&#39;: &#39;177d&#39;, &#39;Device&#39;: &#39;a053&#39;, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} cavium_pkx = {&#39;Class&#39;: &#39;08&#39;, &#39;Vendor&#39;: &#39;177d&#39;, &#39;Device&#39;: &#39;a0dd,a049&#39;, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} cavium_tim = {&#39;Class&#39;: &#39;08&#39;, &#39;Vendor&#39;: &#39;177d&#39;, &#39;Device&#39;: &#39;a051&#39;, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} cavium_zip = {&#39;Class&#39;: &#39;12&#39;, &#39;Vendor&#39;: &#39;177d&#39;, &#39;Device&#39;: &#39;a037&#39;, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} avp_vnic = {&#39;Class&#39;: &#39;05&#39;, &#39;Vendor&#39;: &#39;1af4&#39;, &#39;Device&#39;: &#39;1110&#39;, &#39;SVendor&#39;: None, &#39;SDevice&#39;: None} network_devices = [network_class, cavium_pkx, avp_vnic] crypto_devices = [encryption_class, intel_processor_class] eventdev_devices = [cavium_sso, cavium_tim] mempool_devices = [cavium_fpa] compress_devices = [cavium_zip] # global dict ethernet devices present.</description>
    </item>
    
    <item>
      <title>keepalived应用</title>
      <link>https://workerwork.github.io/posts/keepalived/</link>
      <pubDate>Tue, 30 Jun 2020 14:23:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/keepalived/</guid>
      <description>1. 监控脚本 在Master节点和Slave节点 /etc/keepalived目录下添加check_nginx.sh 文件，用于检测Nginx的存活状况
#!/bin/bash #时间变量，用于记录日志 d=`date --date today +%Y%m%d_%H:%M:%S` #计算nginx进程数量 n=`ps -C nginx --no-heading|wc -l` #如果进程为0，则尝试启动nginx，并且再次检测nginx进程数量， #如果还为0，说明nginx无法启动，此时需要关闭keepalived if [ $n -eq &amp;quot;0&amp;quot; ]; then #如果挂掉了，就启动nginx #注意nginx.conf配置文件的位置 #尝试重新启动nginx /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf sleep 3 #睡眠3秒 n2=`ps -C nginx --no-heading|wc -l` if [ $n2 -eq &amp;quot;0&amp;quot; ]; then #把nginx宕机时间写入日志 echo &amp;quot;$d nginx down,keepalived will stop&amp;quot; &amp;gt;&amp;gt; /usr/local/nginx/logs/check_ng.log #启动失败，将keepalived服务杀死。将vip漂移到其它备份节点 service keepalived stop fi fi  授权: chmod 755 /etc/keepalived/check_nginx.sh
2. 非抢占模式 在Master 节点 /etc/keepalived目录下，配置keepalived.</description>
    </item>
    
    <item>
      <title>网卡命名</title>
      <link>https://workerwork.github.io/posts/eth-name/</link>
      <pubDate>Tue, 30 Jun 2020 10:25:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/eth-name/</guid>
      <description>1. 背景 服务器通常有多块网卡，有板载集成的，同时也有插在PCIe插槽的.
Linux系统的命名原来是eth0,eth1这样的形式，但是这个编号往往不一定准确对应网卡接口的物理顺序.
为解决这类问题，dell开发了biosdevname方案.
systemd v197版本中将dell的方案作了进一步的一般化拓展.
linux内核启动过程中，会默认给网卡以ethX方式随机命名，然后再通过systemd去rename成其他名称.
2. rename流程 step1 依据/usr/lib/udev/rules.d/60-net.rules， 查看是否有ifcfg-xx配置文件（路径在/etc/sysconfig/network-scripts/), 是否有定义了指定MAC地址的配置文件（ifcfg-xx ，xx必须和配置文件的内容DEVICE一致），如果有，则命名改网卡； step2 依据/usr/lib/udev/rules.d/71-biosdevname.rules，如果biosdevname使能了（安装了biosdevname这个包，且内核启动参数显式设置为1）， 且网卡没有在step1中定义，则按照biosdevname命名规则rename网卡；（注意，如果没有安装biosdevname这个包，就没有这个文件） step3, 依据/lib/udev/rules.d/75-net-description.rules，将udev工具会根据device属性将填写网卡的属性命名，可能一个网卡会有多个维度的名称; step4，udev 根据step3中的赋值，按照指定的scheme规则，去给在step1 step2中没有命名的网卡命名; 强调：这个step顺序是在我们没有自定义自己的rules的前提下，如果用户自定义了自己的rules，则用户自定义为优先级最高  3. 命令策略(scheme规则) 1.如果从BIOS中能够取到可用的，板载网卡的索引号，则使用这个索引号命名，例如: eno1，如不能则尝试2 2.如果从BIOS中能够取到可以用的，网卡所在的PCI-E热插拔插槽(注：pci槽位号)的索引号，则使用这个索引号命名，例如: ens1，如不能则尝试3 3.如果能拿到设备所连接的物理位置（PCI总线号+槽位号？）信息，则使用这个信息命名，例如:enp2s0，如不能则尝试4 4.传统的kernel命名方法，例如: eth0，这种命名方法的结果不可预知的，即可能第二块网卡对应eth0，第一块网卡对应eth1 5.使用网卡的MAC地址来命名，这个方法一般不使用 同一个网卡通常同时具有多个维度的名称，systemd在选取的时候，按照有先后次序，使用先命中的 顺序可以简单理解为(eno1-ens1-enp1) root@Bai5gc:/sys/class/net/eth1# udevadm info /sys/class/net/eth1 P: /devices/pci0000:00/0000:00:02.2/0000:03:00.0/net/eth1 E: DEVPATH=/devices/pci0000:00/0000:00:02.2/0000:03:00.0/net/eth1 E: ID_BUS=pci E: ID_MODEL_FROM_DATABASE=Ethernet Connection X552 10 GbE Backplane E: ID_MODEL_ID=0x15ab E: ID_NET_DRIVER=ixgbe E: ID_NET_LINK_FILE=/lib/systemd/network/99-default.link E: ID_NET_NAME_MAC=enxb4a9fca897e7 E: ID_NET_NAME_ONBOARD=eno3 E: ID_NET_NAME_PATH=enp3s0f0 E: ID_PATH=pci-0000:03:00.0 E: ID_PATH_TAG=pci-0000_03_00_0 E: ID_PCI_CLASS_FROM_DATABASE=Network controller E: ID_PCI_SUBCLASS_FROM_DATABASE=Ethernet controller E: ID_VENDOR_FROM_DATABASE=Intel Corporation E: ID_VENDOR_ID=0x8086 E: IFINDEX=3 E: INTERFACE=eth1 E: SUBSYSTEM=net E: SYSTEMD_ALIAS=/sys/subsystem/net/devices/eth1 E: TAGS=:systemd: E: USEC_INITIALIZED=5061037 E: net.</description>
    </item>
    
    <item>
      <title>ubuntu18.04使用xrdp远程桌面</title>
      <link>https://workerwork.github.io/posts/remote-desktop/</link>
      <pubDate>Thu, 11 Jun 2020 09:01:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/remote-desktop/</guid>
      <description>1. 执行下面的脚本安装 http://www.c-nergy.be/downloads/install-xrdp-3.0.zip
root@dongfeng-virtual-machine:/home/dongfeng# cat Install-xrdp-3.0.sh #!/bin/bash ##################################################################################################### # Script_Name : install-xrdp-3.0.sh # Description : Perform a custom installation of xrdp # on ubuntu 18.04 and later # Date : May 2019 # written by : Griffon # Web Site :http://www.c-nergy.be - http://www.c-nergy.be/blog # Version : 3.0 # History : 3.0 - Added support for Ubuntu 19.04 # - New code for Look&#39;n feel using xsessionrc method # - New code for enabling Sound Redirection - compiling from source # - Removed -g parameter # : 2.</description>
    </item>
    
    <item>
      <title>使用rust语言&#43;SDL2库写游戏</title>
      <link>https://workerwork.github.io/posts/rust-game/</link>
      <pubDate>Mon, 08 Jun 2020 15:57:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/rust-game/</guid>
      <description>1. 安装rust curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh  2. 设置rust国内源 #当前用户目录下 /linuxidc/.cargo/ 的.cargo 文件夹，进入.cargo 当前目录，在当前目下创建 config 文件 source.crates-io] registry = &amp;quot;https://github.com/rust-lang/crates.io-index&amp;quot; replace-with = &#39;ustc&#39; [source.ustc] registry = &amp;quot;git://mirrors.ustc.edu.cn/crates.io-index&amp;quot;  3. 安装SDL库 dnf install SDL2 dnf install SDL2-devel dnf install SDL2_image-devel dnf install SDL2_gfx-devel dnf install SDL2_mixer-devel dnf install SDL2_ttf-devel  4. 创建rust项目 cargo new testSDL cd testSDL 编辑cargo.toml，加入sdl2依赖 [dongfeng@localhost testSDL]$ cat Cargo.toml [package] name = &amp;quot;testsdl&amp;quot; version = &amp;quot;0.</description>
    </item>
    
    <item>
      <title>frp内网穿透</title>
      <link>https://workerwork.github.io/posts/frp/</link>
      <pubDate>Thu, 14 May 2020 13:41:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/frp/</guid>
      <description>1. 正常情况一个内网主机与外网服务器的交互 以访问google为例
假设我们的主机IP是192.168.0.10，路由器LAN IP为192.168.0.1，WAN IP为211.22.145.234（这是一个公网IP), google 服务器 IP 为74.125.204.101。 1.主机构建HTTP请求数据包，目标IP为74.125.204.101，目标端口是80/443，源IP为192.168.0.10，源端口随机生成，假定为5000 2.主机检查目标IP地址，发现不在一个网段，数据包丢给默认网关192.168.0.1 3.路由器LAN口收到数据包，构建NAT映射，随机生成端口，假定为5500,这样映射就是：5500-&amp;gt;192.168.0.10:5000． WIN口收到的数据包，如果目标端口是5500,则会转发给192.168.0.10的5000端口 4.路由器修改数据包的源端口为5500,源ＩＰ地址为211.22.145.234，使用WAN口将数据包发出去 5.google服务器收到请求，构建响应HTTP数据包，目标IP地址为211.22.145.234,目标端口是5500 6.路由器WAN口收到数据包，目标端口是5500,查询NAT表，发现对应的机器是192.168.0.10:5000， 所以修改目标IP为192.168.0.10，目标端口为5000，并通过LAN口发送给主机 7.主机收到数据包，完成一次通信  2. 内网穿透实现 测试服务器没有公网IP，想要让外网直接调用内网的服务，就需要用到内网穿透.
和使用路由器与外网交互类似，需要有一个第三方拥有公网IP的服务器进行路由的中转，代替路由器的角色.
由内网服务器主动请求公网服务器，建立一个长连接，这时公网服务器就可以随时随地的向内网服务器发送消息了
当使用浏览器想要访问内网服务时，先将请求发送到公网服务器上，公网服务器再通过之前建立的长连接将请求发送到内网服务器中。
从而实现在外网请求内网服务的需求
3. 公网服务器设置 # wget https://github.com/fatedier/frp/releases/download/v0.14.1/frp_0.14.1_linux_amd64.tar.gz # sudo tar zxf frp_0.14.1_linux_amd64.tar.gz # cd frp_0.14.1_linux_amd64/ # sudo vim frps.ini # [common] bind_port = 8989 # frp服务的端口 vhost_http_port = 8889 # frp的http服务的端口 # 启动服务 # ./frps -c frps.ini # 前台直接启动，测试看日志方便 # nohup ./frps -c ./frps.ini &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 &amp;amp; # 后台运行  4.</description>
    </item>
    
    <item>
      <title>p4实践环境</title>
      <link>https://workerwork.github.io/posts/p4/</link>
      <pubDate>Wed, 06 May 2020 18:00:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/p4/</guid>
      <description>1. 安装ubuntu16.04(18.04)并更新依赖 # sudo apt update # sudo apt-get install automake cmake libjudy-dev libpcap-dev libboost-dev \ libboost-test-dev libboost-program-options-dev libboost-system-dev \ libboost-filesystem-dev libboost-thread-dev libevent-dev libtool \ flex bison pkg-config g++ libssl-dev -y # sudo apt-get install cmake g++ git automake libtool libgc-dev bison flex libfl-dev \ libgmp-dev libboost-dev libboost-iostreams-dev libboost-graph-dev \ llvm pkg-config python python-scapy python-ipaddr python-ply tcpdump curl -y # sudo apt-get install libreadline6 libreadline6-dev python-pip python-scapy -y # sudo pip install psutil # sudo pip install crcmod  2.</description>
    </item>
    
    <item>
      <title>github加速</title>
      <link>https://workerwork.github.io/posts/github-fast/</link>
      <pubDate>Tue, 05 May 2020 16:17:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/github-fast/</guid>
      <description>1. 给git设置socks5/vmess代理 前提是开启代理服务，可以使用V2rayL: https://github.com/jiangxufeng/v2rayL 和 ghelper
使用 https 的时候，就是使用 https 协议复制仓库的时候
git config --global http.proxy &#39;socks5://127.0.0.1:1080&#39; git config --global https.proxy &#39;socks5://127.0.0.1:1080&#39; git config --global http.proxy &#39;vmess://127.0.0.1:1081&#39; git config --global https.proxy &#39;vmess://127.0.0.1:1081&#39;  也可以直接修改用户主目录下的 .gitconfig 文件
[http] proxy = socks5://127.0.0.1:1080 [https] proxy = socks5://127.0.0.1:1080 [http] proxy = vmess://127.0.0.1:1081 [https] proxy = vmess://127.0.0.1:1081  取消代理
git config --global --unset http.proxy git config --global --unset https.proxy  查看已有代理
git config --global -l  在使用 git 开头的路径时，也就是在使用 ssh 通道时</description>
    </item>
    
    <item>
      <title>vpp node-graph编排过程</title>
      <link>https://workerwork.github.io/posts/vpp-node-graph/</link>
      <pubDate>Thu, 30 Apr 2020 10:14:26 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-node-graph/</guid>
      <description>1. vpp node graph VPP处理报文时是沿着一个有向图进行处理的，每一个功能单元称之为节点(node)
2. 数据结构 静态数据结构 节点全局管理结构vlib_node_main_t
typedef struct { /* Public nodes. */ /* 节点指针数组，使用下标作为索引 */ vlib_node_t **nodes; /* Node index hashed by node name. */ /* 根据节点名字进行hash，可以根据节点名字进行hash表查找 * 只有main线程才会委会该hash表 */ uword *node_by_name; u32 flags; /* 该标志表示Runtime信息已经被初始化过了 */ #define VLIB_NODE_MAIN_RUNTIME_STARTED (1 &amp;lt;&amp;lt; 0) /* Nodes segregated by type for cache locality. Does not apply to nodes of type VLIB_NODE_TYPE_INTERNAL. */ vlib_node_runtime_t *nodes_by_type[VLIB_N_NODE_TYPE]; /* Node runtime indices for input nodes with pending interrupts.</description>
    </item>
    
    <item>
      <title>vpp 节点报文处理流程分析</title>
      <link>https://workerwork.github.io/posts/vpp-node-fw/</link>
      <pubDate>Wed, 29 Apr 2020 11:14:24 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-node-fw/</guid>
      <description>1. 以sample例子来分析vpp节点对报文的处理流程 vpp/src/examples/sample-plugin/sample $ll total 56 -rw-rw-r-- 1 ych ych 886 Apr 1 17:34 CMakeLists.txt -rw-rw-r-- 1 ych ych 17933 Apr 1 17:34 node.c -rw-rw-r-- 1 ych ych 712 Apr 1 17:34 sample_all_api_h.h -rw-rw-r-- 1 ych ych 1068 Apr 1 17:34 sample.api -rw-rw-r-- 1 ych ych 6569 Apr 1 17:34 sample.c -rw-rw-r-- 1 ych ych 1135 Apr 1 17:34 sample.h -rw-rw-r-- 1 ych ych 960 Apr 1 17:34 sample_msg_enum.h -rw-rw-r-- 1 ych ych 5512 Apr 1 17:34 sample_test.</description>
    </item>
    
    <item>
      <title>vpp sample plugin</title>
      <link>https://workerwork.github.io/posts/vpp-sample-plugin/</link>
      <pubDate>Tue, 28 Apr 2020 17:25:50 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-sample-plugin/</guid>
      <description>1. plugin_sample.c 在此文件中定义feature和cli
#include &amp;lt;vnet/plugin/plugin.h&amp;gt; #include &amp;lt;plugin_sample/plugin_sample.h&amp;gt; plugin_sample_main_t plugin_sample_main; //开关实现 int plugin_sample_enable_disable(u32 sw_if_index, //index int enable_disable)	//开关标识 { vnet_sw_interface_t *sw; int ret = 0; /* Utterly wrong? */ if (pool_is_free_index (plugin_sample_main.vnet_main-&amp;gt;interface_main.sw_interfaces, //vnet_main结构中的interface_main结构中的sw接口 sw_if_index)) //接口索引 return VNET_API_ERROR_INVALID_SW_IF_INDEX; /* Not a physical port? */ sw = vnet_get_sw_interface(plugin_sample_main.vnet_main,	//vnet_main结构 sw_if_index);	if (sw-&amp;gt;type != VNET_SW_INTERFACE_TYPE_HARDWARE) return VNET_API_ERROR_INVALID_SW_IF_INDEX; vnet_feature_enable_disable(&amp;quot;ip4-unicast&amp;quot;, //挂载节点 &amp;quot;plugin_sample&amp;quot;, sw_if_index, enable_disable, 0, 0); return ret; } static clib_error_t* plugin_sample_enable_disable_command_fn(vlib_main_t* vm,	//vlib_main结构 unformat_input_t *input, vlib_cli_command_t *cmd) { u32 sw_if_index = ~0;	//~0 取反= 1 int enable_disable = 1; while(unformat_check_input(input) !</description>
    </item>
    
    <item>
      <title>WCG-deps-install</title>
      <link>https://workerwork.github.io/posts/wcg-deps/</link>
      <pubDate>Tue, 28 Apr 2020 09:45:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/wcg-deps/</guid>
      <description>1. 下载对应的依赖包 # yumdownloader --downloadonly --downloaddir=. xxx # 提取包内容 # rpm2cpio *.rpm | cpio -div  2. install.sh #!/bin/bash - ########################################################## # wcg-deps-install.sh # version:1.0 # update:20181120 ########################################################## DIR=&amp;quot;/home/wcg/WCG-deps&amp;quot; function wcg_deps() { for dir in curl fcgi gsoap lksctp lrzsz vconfig tftp redis spawn-fcgi keepalived openssh nginx net-tools libevent do cd $DIR/$dir &amp;amp;&amp;amp; rpm -Uvh *.rpm --force --nodeps &amp;amp; done } function segw_deps() { for dir in segw do cd $DIR/$dir &amp;amp;&amp;amp; rpm -Uvh *.</description>
    </item>
    
    <item>
      <title>kernel 字符设备驱动</title>
      <link>https://workerwork.github.io/posts/kernel-dev/</link>
      <pubDate>Wed, 22 Apr 2020 15:14:50 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/kernel-dev/</guid>
      <description>1. 字符设备 Linux字符设备是一种按字节来访问的设备，字符驱动则负责驱动字符设备，这样的驱动通常实现open、close、read和write系统调用。例如：串口、Led、按键等
通过字符设备文件（/dev/xxx），应用程序可以使用相应的字符设备驱动来控制字符设备
2. 如何创建字符设备  使用命令mknod : mknod /dev/文件名 c 主设备号 次设备号 （查看主设备号：cat /proc/devices） 使用函数创建：mknod()
int mknod(const char *pathname, mode_t mode, dev_t dev);   3. 文件系统与字符设备驱动程序之间的关系  在Linux系统中，每一个打开的文件，在内核中都会关联一个struct file结构，它是由内核在打开文件时创建，在文件关闭后释放。
struct file结构中的重要成员 * struct file_operations* f_op;　//文件操作函数集 * loff_t f_pos;　//文件读写指针  每一个存在于文件系统中的文件都会关联一个inode结构，该结构主要用来记录文件物理上的信息。因此，它和代表打开文件的file结构是不同的，一个文件没有被打开时不会关联file结构，但是会关联一个inode结构（存于磁盘，操作文件时在内存中建立相应的映射结构）
  注：inode用于存储文件的元信息（除了文件名的所有信息），中文译名索引节点
 从上图可知，系统实质上是把字符设备的注册表看成了文件。其中chrdevs[]在内核的定义如下
static struct char_device_struct { struct char_device_struct *next; unsigned int major; unsigned int baseminor; int minorct; char name[64]; struct cdev *cdev; /* will die */ } *chrdevs[CHRDEV_MAJOR_HASH_SIZE];   4.</description>
    </item>
    
    <item>
      <title>cpu亲和性</title>
      <link>https://workerwork.github.io/posts/affinity/</link>
      <pubDate>Mon, 20 Apr 2020 16:17:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/affinity/</guid>
      <description>1. 什么是cpu亲和性(affinity) CPU的亲和性， 就是进程要在指定的 CPU 上尽量长时间地运行而不被迁移到其他处理器，也称为CPU关联性； 再简单的点的描述就将制定的进程或线程绑定到相应的cpu上； 在多核运行的机器上，每个CPU本身自己会有缓存，缓存着进程使用的信息，而进程可能会被OS调度到其他CPU上， 如此，CPU cache命中率就低了，当绑定CPU后，程序就会一直在指定的cpu跑，不会由操作系统调度到其他CPU上，性能有一定的提高。
软亲和性(affinity): 就是进程要在指定的CPU上尽量长时间地运行而不被迁移到其他处理器，Linux内核进程调度器天生就具有被称为软CPU亲和性(affinity) 的特性，这意味着进程通常不会在处理器之间频繁迁移。 这种状态正是我们希望的，因为进程迁移的频率小就意味着产生的负载小。
硬亲和性(affinity): 简单来说就是利用linux内核提供给用户的API，强行将进程或者线程绑定到某一个指定的cpu核运行。
解释: 在linux内核中，所有的进程都有一个相关的数据结构，称为 task_struct。这个结构非常重要，原因有很多；其中与 亲和性（affinity）相关度最高的是 cpus_allowed 位掩码。 这个位掩码由 n 位组成，与系统中的 n 个逻辑处理器一一对应。 具有 4 个物理 CPU 的系统可以有 4 位。如果这些 CPU 都启用了超线程，那么这个系统就有一个 8 位的位掩码。 如果为给定的进程设置了给定的位，那么这个进程就可以在相关的 CPU 上运行。因此，如果一个进程可以在任何 CPU 上运行，并且能够根据需要在处理器之间进行迁移，那么位掩码就全是 1。 实际上，这就是 Linux 中进程的缺省状态;（这部分内容在这个博客中有提到一点：http://www.cnblogs.com/wenqiang/p/4802619.html）
cpus_allowed用于控制进程可以在哪里处理器上运行
sched_set_affinity() （用来修改位掩码）
sched_get_affinity() （用来查看当前的位掩码）
2. 进程与cpu的绑定 sched_setaffinity可以将某个进程绑定到一个特定的CPU。你比操作系统更了解自己的程序，为了避免调度器愚蠢的调度你的程序，或是为了在多线程程序中避免缓存失效造成的开销，你可能会希望这样做
在进行进程与cpu的绑定前，我们先了解编写程序需要准备的知识点
SCHED_SETAFFINITY(2) Linux Programmer&#39;s Manual SCHED_SETAFFINITY(2) NAME sched_setaffinity, sched_getaffinity - set and get a process&#39;s CPU affinity mask SYNOPSIS #define _GNU_SOURCE /* See feature_test_macros(7) */ #include &amp;lt;sched.</description>
    </item>
    
    <item>
      <title>kernel module编程</title>
      <link>https://workerwork.github.io/posts/kernel-module/</link>
      <pubDate>Fri, 17 Apr 2020 10:33:52 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/kernel-module/</guid>
      <description>1. Linux Kernel Module是什么 Linux Kernel Module是一段可以在运行时被加载到Linux Kernel中的代码，可以使用Kernel Functions。Linux Kernel Module的用途很广，最常见的例子就是Device Driver，也就是设备驱动程序。
如果没有Linux Kernel Module，每一行修改Kernel代码，每一个新增的Kernel功能特性，都需要重新编译Kernel，大大浪费了时间和效率。
2. kernel module编程 [root@localhost test]# ll 总用量 16 -rw-r--r--. 1 root root 728 4月 17 10:28 hello.c -rw-r--r--. 1 root root 229 4月 17 10:24 Makefile -rw-r--r--. 1 root root 190 4月 17 10:26 mymax.c -rw-r--r--. 1 root root 70 4月 17 10:17 mymax.h  [root@localhost test]# cat hello.c #include &amp;lt;linux/init.h&amp;gt;	/* Needed for the module-macros */ #include &amp;lt;linux/module.</description>
    </item>
    
    <item>
      <title>linux内核定制</title>
      <link>https://workerwork.github.io/posts/self-kernel/</link>
      <pubDate>Tue, 14 Apr 2020 16:20:00 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/self-kernel/</guid>
      <description>1. 下载内核源代码 从 http://www.kernel.org 下载内核源代码RPM包 例如linux-2.6.27.62.tar.bz2
2. 解压内核 # bzip2 -d linux-2.6.27.62.tar.bz2 # tar -xvf linux-2.6.27.62.tar  3. 定制内核 #定制内核有很多种方法：make config(最基本方法),make defconfig（默认的方法) # make config # make defconfig # make menuconfig #会生成.config文件，这个文件也可以从/boot路径下拷贝 #Y是该选项能够构建到内核内部 #M是构建模块  4. 构建内核 # make clean //这一步最好执行一下 # make -j2  5. 打包成rpm # make rpm  6. 安装并引导内核 # make modules_install //安装模块 # make install //安装内核 #这时，系统会自动在你的启动菜单中加入启动新内核的菜单,如 [root@localhost linux-2.6.27.62]# cat /boot/grub/menu.lst default=1 timeout=5 splashimage=(hd0,0)/grub/splash.xpm.gz hiddenmenu title Red Hat Enterprise Linux AS (2.</description>
    </item>
    
    <item>
      <title>制作CentOS ISO</title>
      <link>https://workerwork.github.io/posts/centos-iso/</link>
      <pubDate>Wed, 28 Aug 2019 16:37:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/centos-iso/</guid>
      <description>1. 复制光盘文件 1）挂载iso镜像 #创建目录用于挂载光盘 mkdir /root/centos7 #挂载iso镜像 mount -o loop CentOS-7.0-1406-x86_64-DVD.iso /root/centos7  2）复制光盘文件到编辑目录进行编辑 因为挂载上iso镜像是只读的，如果要编辑，需要将文件复制出来，再编辑。
#首先创建编辑目录： mkdir /root/centos7_iso #复制光盘文件： cp -rf /root/centos7/* /root/centos7_iso/ #diskinfo treeinfo文件需单独拷贝下： cp /root/centos7/.discinfo /root/centos7_iso/ cp /root/centos7/.treeinfo /root/centos7_iso/  2. 编辑ks.cfg文件 系统安装的时候，按照ks.cfg文件的内容进行安装，我们把ks.cfg文件放到isolinux目录下：
cd /root/centos7_iso/isolinux  vim ks-init.cfg #platform=x86, AMD64, or Intel EM64T #version=DEVEL # Install OS instead of upgrade install # Keyboard layouts keyboard &#39;us&#39; # Root password rootpw --iscrypted $1$JtB/A66X$GCT7X3FCJVAPGd3sEY0mx0 # System language lang en_US # System authorization information auth --useshadow --passalgo=sha512 # Use cdrom installation media cdrom # Use text mode install #text graphical # SELinux configuration selinux --disabled # Do not configure the X Window System skipx #firstboot --enable #ignoredisk --only-use=sda # Firewall configuration firewall --disabled # Network information network --bootproto=dhcp --device=eth0 --onboot=no network --hostname=localhost.</description>
    </item>
    
  </channel>
</rss>