<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vpp on WorkSpace</title>
    <link>https://workerwork.github.io/tags/vpp/</link>
    <description>Recent content in vpp on WorkSpace</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 29 Apr 2020 16:04:24 +0800</lastBuildDate>
    
	<atom:link href="https://workerwork.github.io/tags/vpp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>vpp feature节点以及数据走向控制</title>
      <link>https://workerwork.github.io/posts/vpp-feature/</link>
      <pubDate>Wed, 29 Apr 2020 16:04:24 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-feature/</guid>
      <description>1. feature介绍 早期的VPP本身的node框架比较固定，各个node之间逻辑连接已经固化，为此新版本增加了feature机制
这里feature机制本质上来说还是结点，只不过该结点可以在运行的时候通过命令进行配置是否打开或关闭，从而影响数据流的走向
2. 选择合适的arc类 对新加入的结点进行管理，新的feature(即我们新建的结点)必须属于某个arc类，并作用于某个interface实体
通过set interface feature   arc  [disable]命令来开启或关闭该feature功能
通常arc类的名字对应为其起点结点的名字，使用命令开启关闭feature功能能动态的改变数据的流向
如果选择按照feature机制来加入结点的话需要注意以下几点：
VPP提供的arc类比较多，我们需要自己选择合适的arc来插入我们的结点:
1.nsh-output: 2.mpls-output: 3.mpls-input: 4.ip6-drop: 5.ip6-punt: 6.ip6-local: 7.ip6-output: 8.ip6-multicast: 9.ip6-unicast: 10.ip4-drop: 11.ip4-punt: 12.ip4-local: 13.ip4-output: 14.ip4-multicast: 15.ip4-unicast: 16.ethernet-output: 17.interface-output: 18.device-input:  ip4-unicast arc
文件：vnet/ip/ip4_forward.c /* Built-in ip4 unicast rx feature path definition */ /* *INDENT-OFF* */ VNET_FEATURE_ARC_INIT (ip4_unicast, static) = { .arc_name = &amp;quot;ip4-unicast&amp;quot;, /*arc 类 名字*/ .start_nodes = VNET_FEATURES (&amp;quot;ip4-input&amp;quot;, &amp;quot;ip4-input-no-checksum&amp;quot;), /*开始node*/ .</description>
    </item>
    
    <item>
      <title>vpp 节点报文处理流程分析</title>
      <link>https://workerwork.github.io/posts/vpp-node-fw/</link>
      <pubDate>Wed, 29 Apr 2020 11:14:24 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-node-fw/</guid>
      <description>1. 以sample例子来分析vpp节点对报文的处理流程 vpp/src/examples/sample-plugin/sample $ll total 56 -rw-rw-r-- 1 ych ych 886 Apr 1 17:34 CMakeLists.txt -rw-rw-r-- 1 ych ych 17933 Apr 1 17:34 node.c -rw-rw-r-- 1 ych ych 712 Apr 1 17:34 sample_all_api_h.h -rw-rw-r-- 1 ych ych 1068 Apr 1 17:34 sample.api -rw-rw-r-- 1 ych ych 6569 Apr 1 17:34 sample.c -rw-rw-r-- 1 ych ych 1135 Apr 1 17:34 sample.h -rw-rw-r-- 1 ych ych 960 Apr 1 17:34 sample_msg_enum.h -rw-rw-r-- 1 ych ych 5512 Apr 1 17:34 sample_test.</description>
    </item>
    
    <item>
      <title>vpp sample plugin</title>
      <link>https://workerwork.github.io/posts/vpp-sample-plugin/</link>
      <pubDate>Tue, 28 Apr 2020 17:25:50 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp-sample-plugin/</guid>
      <description>1. plugin_sample.c 在此文件中定义feature和cli
#include &amp;lt;vnet/plugin/plugin.h&amp;gt; #include &amp;lt;plugin_sample/plugin_sample.h&amp;gt; plugin_sample_main_t plugin_sample_main; //开关实现 int plugin_sample_enable_disable(u32 sw_if_index, //index int enable_disable)	//开关标识 { vnet_sw_interface_t *sw; int ret = 0; /* Utterly wrong? */ if (pool_is_free_index (plugin_sample_main.vnet_main-&amp;gt;interface_main.sw_interfaces, //vnet_main结构中的interface_main结构中的sw接口 sw_if_index)) //接口索引 return VNET_API_ERROR_INVALID_SW_IF_INDEX; /* Not a physical port? */ sw = vnet_get_sw_interface(plugin_sample_main.vnet_main,	//vnet_main结构 sw_if_index);	if (sw-&amp;gt;type != VNET_SW_INTERFACE_TYPE_HARDWARE) return VNET_API_ERROR_INVALID_SW_IF_INDEX; vnet_feature_enable_disable(&amp;quot;ip4-unicast&amp;quot;, //挂载节点 &amp;quot;plugin_sample&amp;quot;, sw_if_index, enable_disable, 0, 0); return ret; } static clib_error_t* plugin_sample_enable_disable_command_fn(vlib_main_t* vm,	//vlib_main结构 unformat_input_t *input, vlib_cli_command_t *cmd) { u32 sw_if_index = ~0;	//~0 取反= 1 int enable_disable = 1; while(unformat_check_input(input) !</description>
    </item>
    
    <item>
      <title>基于VPP&#43;DPDK开源框架开发UPF</title>
      <link>https://workerwork.github.io/posts/vpp/</link>
      <pubDate>Fri, 01 Nov 2019 16:11:51 +0800</pubDate>
      
      <guid>https://workerwork.github.io/posts/vpp/</guid>
      <description>DPDK DPDK是什么 DPDK全称为Date plane development kit，是一个用来进行包数据处理加速的软件库。
为什么DPDK 传统 Linux 内核网络数据流程：
硬件中断---&amp;gt;取包分发至内核线程---&amp;gt;软件中断---&amp;gt;内核线程在协议栈中处理包---&amp;gt;处理完毕通知用户层 用户层收包--&amp;gt;网络层---&amp;gt;逻辑层---&amp;gt;业务层  基于传统 OS 内核的数据处理的弊端：
 中断处理。当网络中大量数据包到来时，会产生频繁的硬件中断请求，这些硬件中断可以打断之前较低优先级的软中断或者系统调用的执行过程，如果这种打断频繁的话，将会产生较高的性能开销。
 内存拷贝。正常情况下，一个网络数据包从网卡到应用程序需要经过如下的过程：数据从网卡通过 DMA 等方式传到内核开辟的缓冲区，然后从内核空间拷贝到用户态空间，在 Linux 内核协议栈中，这个耗时操作甚至占到了数据包整个处理流程的 57.1%。
 上下文切换。频繁到达的硬件中断和软中断都可能随时抢占系统调用的运行，这会产生大量的上下文切换开销。另外，在基于多线程的服务器设计框架中，线程间的调度也会产生频繁的上下文切换开销，同样，锁竞争的耗能也是一个非常严重的问题。
 局部性失效。如今主流的处理器都是多个核心的，这意味着一个数据包的处理可能跨多个 CPU 核心，比如一个数据包可能中断在 cpu0，内核态处理在 cpu1，用户态处理在 cpu2，这样跨多个核心，容易造成 CPU 缓存失效，造成局部性失效。如果是 NUMA 架构，更会造成跨 NUMA 访问内存，性能受到很大影响。
 内存管理。传统服务器内存页为 4K，为了提高内存的访问速度，避免 cache miss，可以增加 cache 中映射表的条目，但这又会影响 CPU 的检索效率。
  如何改进：
 控制层和数据层分离。将数据包处理、内存管理、处理器调度等任务转移到用户空间去完成，而内核仅仅负责部分控制指令的处理。这样就不存在上述所说的系统中断、上下文切换、系统调用、系统调度等等问题。
 使用多核编程技术代替多线程技术，并设置 CPU 的亲和性，将线程和 CPU 核进行一比一绑定，减少彼此之间调度切换。
 针对 NUMA 系统，尽量使 CPU 核使用所在 NUMA 节点的内存，避免跨内存访问。
 使用大页内存代替普通的内存，减少 cache-miss。</description>
    </item>
    
  </channel>
</rss>